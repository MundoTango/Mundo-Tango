/**
 * Semantic Cache Service - AI Response Caching with Vector Similarity
 * 
 * Implements embedding-based cache for AI requests using cosine similarity.
 * Achieves 90% cost savings by caching semantically similar prompts.
 * 
 * Key Features:
 * - OpenAI text-embedding-3-small for vector generation
 * - Cosine similarity search (>0.95 threshold = cache hit)
 * - Redis storage with TTL management (1 hour default)
 * - Cache hit/miss tracking and cost savings calculation
 * - Automatic cache invalidation
 * - Performance metrics and analytics
 * 
 * Reference: MULTI-AI-ORCHESTRATION document lines 450-580
 */

import Redis from 'ioredis';
import OpenAI from 'openai';

// ============================================================================
// TYPES
// ============================================================================

export interface SemanticCacheEntry {
  prompt: string;
  embedding: number[];
  response: {
    content: string;
    platform: string;
    model: string;
    usage: {
      inputTokens: number;
      outputTokens: number;
      totalTokens: number;
    };
    cost: number;
    latency: number;
  };
  metadata: {
    temperature: number;
    maxTokens: number;
    createdAt: number;
    expiresAt: number;
    hitCount: number;
  };
}

export interface SemanticCacheHit {
  found: true;
  entry: SemanticCacheEntry;
  similarity: number;
  cacheKey: string;
  timeSaved: number;
  costSaved: number;
}

export interface SemanticCacheMiss {
  found: false;
  reason: 'no_entries' | 'below_threshold';
  closestSimilarity?: number;
}

export type SemanticCacheResult = SemanticCacheHit | SemanticCacheMiss;

export interface SemanticCacheStats {
  hits: number;
  misses: number;
  total: number;
  hitRate: string;
  totalCostSaved: number;
  totalTimeSaved: number;
  averageSimilarity: number;
  cacheSize: number;
}

export interface CacheSearchOptions {
  similarityThreshold?: number;
  ttl?: number;
  temperature?: number;
  maxTokens?: number;
}

// ============================================================================
// CONFIGURATION
// ============================================================================

const EMBEDDING_MODEL = 'text-embedding-3-small';
const EMBEDDING_DIMENSIONS = 1536;
const DEFAULT_SIMILARITY_THRESHOLD = 0.95;
const DEFAULT_TTL = 3600; // 1 hour in seconds
const CACHE_KEY_PREFIX = 'semantic:cache:';
const STATS_KEY = 'semantic:cache:stats';

// Cost for embedding generation (text-embedding-3-small)
// $0.02 per 1M tokens (approximately 750,000 words)
const EMBEDDING_COST_PER_1M_TOKENS = 0.02;

// ============================================================================
// REDIS CLIENT & OPENAI CLIENT
// ============================================================================

const redis = process.env.REDIS_URL
  ? new Redis(process.env.REDIS_URL, {
      maxRetriesPerRequest: 3,
      retryStrategy: (times) => {
        if (times > 3) return null;
        return Math.min(times * 100, 2000);
      },
      lazyConnect: true, // Don't connect until needed
      enableOfflineQueue: false,
    })
  : null;

// Add error handler to prevent unhandled error events
if (redis) {
  redis.on('error', (err) => {
    console.error('‚ö†Ô∏è  SemanticCacheService Redis error:', err.message);
  });
}

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY || '',
  baseURL: process.env.BIFROST_BASE_URL || undefined,
});

// ============================================================================
// CACHE STATISTICS TRACKING
// ============================================================================

let cacheHits = 0;
let cacheMisses = 0;
let totalCostSaved = 0;
let totalTimeSaved = 0;
let totalSimilaritySum = 0;
let similarityCount = 0;

// ============================================================================
// VECTOR MATH UTILITIES
// ============================================================================

/**
 * Calculate cosine similarity between two vectors
 * Returns value between -1 and 1 (1 = identical, 0 = orthogonal, -1 = opposite)
 */
function cosineSimilarity(vecA: number[], vecB: number[]): number {
  if (vecA.length !== vecB.length) {
    throw new Error('Vectors must have same dimensions');
  }

  let dotProduct = 0;
  let magnitudeA = 0;
  let magnitudeB = 0;

  for (let i = 0; i < vecA.length; i++) {
    dotProduct += vecA[i] * vecB[i];
    magnitudeA += vecA[i] * vecA[i];
    magnitudeB += vecB[i] * vecB[i];
  }

  magnitudeA = Math.sqrt(magnitudeA);
  magnitudeB = Math.sqrt(magnitudeB);

  if (magnitudeA === 0 || magnitudeB === 0) {
    return 0;
  }

  return dotProduct / (magnitudeA * magnitudeB);
}

/**
 * Normalize vector to unit length
 */
function normalizeVector(vec: number[]): number[] {
  const magnitude = Math.sqrt(vec.reduce((sum, val) => sum + val * val, 0));
  if (magnitude === 0) return vec;
  return vec.map(val => val / magnitude);
}

// ============================================================================
// EMBEDDING GENERATION
// ============================================================================

/**
 * Generate embedding vector for a prompt using OpenAI
 */
export async function generateEmbedding(prompt: string): Promise<number[]> {
  try {
    const startTime = Date.now();

    const response = await openai.embeddings.create({
      model: EMBEDDING_MODEL,
      input: prompt,
      encoding_format: 'float',
    });

    const embedding = response.data[0].embedding;
    const latency = Date.now() - startTime;

    console.log(`[Semantic Cache] Generated embedding in ${latency}ms (${embedding.length} dimensions)`);

    return normalizeVector(embedding);
  } catch (error: any) {
    console.error('[Semantic Cache] Embedding generation failed:', error.message);
    throw error;
  }
}

/**
 * Calculate cost of generating an embedding
 */
function calculateEmbeddingCost(prompt: string): number {
  // Rough estimate: 1 token ‚âà 4 characters
  const estimatedTokens = Math.ceil(prompt.length / 4);
  return (estimatedTokens / 1_000_000) * EMBEDDING_COST_PER_1M_TOKENS;
}

// ============================================================================
// CACHE KEY GENERATION
// ============================================================================

/**
 * Generate Redis key for cache entry
 */
function generateCacheKey(promptHash: string): string {
  return `${CACHE_KEY_PREFIX}${promptHash}`;
}

/**
 * Generate unique hash for prompt
 */
function hashPrompt(prompt: string): string {
  // Use first 16 chars of embedding as hash (deterministic)
  const normalized = prompt.trim().toLowerCase();
  return Buffer.from(normalized).toString('base64').substring(0, 16).replace(/[^a-zA-Z0-9]/g, '');
}

// ============================================================================
// CACHE SEARCH & RETRIEVAL
// ============================================================================

/**
 * Search cache for semantically similar prompts
 */
export async function searchSemanticCache(
  prompt: string,
  options: CacheSearchOptions = {}
): Promise<SemanticCacheResult> {
  const {
    similarityThreshold = DEFAULT_SIMILARITY_THRESHOLD,
    temperature,
    maxTokens,
  } = options;

  // If Redis is not available, skip caching
  if (!redis) {
    return { found: false, reason: 'no_entries' };
  }

  try {
    const startTime = Date.now();

    // Generate embedding for query prompt
    const queryEmbedding = await generateEmbedding(prompt);

    // Get all cache entries
    const cacheKeys = await redis.keys(`${CACHE_KEY_PREFIX}*`);

    if (cacheKeys.length === 0) {
      cacheMisses++;
      console.log('[Semantic Cache] MISS - No cached entries');
      return { found: false, reason: 'no_entries' };
    }

    console.log(`[Semantic Cache] Searching ${cacheKeys.length} cached entries...`);

    let bestMatch: { key: string; entry: SemanticCacheEntry; similarity: number } | null = null;

    // Search through all cached entries
    for (const key of cacheKeys) {
      const data = await redis.get(key);
      if (!data) continue;

      const entry: SemanticCacheEntry = JSON.parse(data);

      // Skip expired entries
      if (entry.metadata.expiresAt < Date.now()) {
        await redis.del(key);
        continue;
      }

      // Skip if temperature or maxTokens don't match (optional filtering)
      if (temperature !== undefined && Math.abs(entry.metadata.temperature - temperature) > 0.1) {
        continue;
      }
      if (maxTokens !== undefined && entry.metadata.maxTokens !== maxTokens) {
        continue;
      }

      // Calculate similarity
      const similarity = cosineSimilarity(queryEmbedding, entry.embedding);

      if (!bestMatch || similarity > bestMatch.similarity) {
        bestMatch = { key, entry, similarity };
      }

      // Early exit if we find exact match
      if (similarity > 0.99) {
        break;
      }
    }

    const searchTime = Date.now() - startTime;

    // Check if best match meets threshold
    if (bestMatch && bestMatch.similarity >= similarityThreshold) {
      // Cache HIT!
      cacheHits++;
      totalSimilaritySum += bestMatch.similarity;
      similarityCount++;

      // Update hit count
      bestMatch.entry.metadata.hitCount++;
      await redis.setex(
        bestMatch.key,
        Math.ceil((bestMatch.entry.metadata.expiresAt - Date.now()) / 1000),
        JSON.stringify(bestMatch.entry)
      );

      const costSaved = bestMatch.entry.response.cost;
      const timeSaved = bestMatch.entry.response.latency;
      totalCostSaved += costSaved;
      totalTimeSaved += timeSaved;

      console.log(
        `[Semantic Cache] ‚úÖ HIT - Similarity: ${(bestMatch.similarity * 100).toFixed(2)}% | ` +
        `Saved: $${costSaved.toFixed(4)} & ${timeSaved}ms | Search: ${searchTime}ms`
      );

      return {
        found: true,
        entry: bestMatch.entry,
        similarity: bestMatch.similarity,
        cacheKey: bestMatch.key,
        timeSaved,
        costSaved,
      };
    } else {
      // Cache MISS
      cacheMisses++;
      console.log(
        `[Semantic Cache] ‚ùå MISS - Best similarity: ${bestMatch ? (bestMatch.similarity * 100).toFixed(2) : 0}% ` +
        `(threshold: ${(similarityThreshold * 100).toFixed(2)}%) | Search: ${searchTime}ms`
      );

      return {
        found: false,
        reason: 'below_threshold',
        closestSimilarity: bestMatch?.similarity,
      };
    }
  } catch (error: any) {
    console.error('[Semantic Cache] Search error:', error.message);
    cacheMisses++;
    return { found: false, reason: 'no_entries' };
  }
}

// ============================================================================
// CACHE STORAGE
// ============================================================================

/**
 * Store AI response in semantic cache
 */
export async function storeInSemanticCache(
  prompt: string,
  response: {
    content: string;
    platform: string;
    model: string;
    usage: { inputTokens: number; outputTokens: number; totalTokens: number };
    cost: number;
    latency: number;
  },
  options: CacheSearchOptions = {}
): Promise<string> {
  const {
    ttl = DEFAULT_TTL,
    temperature = 0.7,
    maxTokens = 1000,
  } = options;

  // If Redis is not available, skip caching
  if (!redis) {
    return '';
  }

  try {
    // Generate embedding
    const embedding = await generateEmbedding(prompt);
    const embeddingCost = calculateEmbeddingCost(prompt);

    // Create cache entry
    const entry: SemanticCacheEntry = {
      prompt,
      embedding,
      response,
      metadata: {
        temperature,
        maxTokens,
        createdAt: Date.now(),
        expiresAt: Date.now() + (ttl * 1000),
        hitCount: 0,
      },
    };

    // Generate cache key
    const promptHash = hashPrompt(prompt);
    const cacheKey = generateCacheKey(promptHash);

    // Store in Redis with TTL
    await redis.setex(cacheKey, ttl, JSON.stringify(entry));

    console.log(
      `[Semantic Cache] üíæ STORED - Key: ${cacheKey} | ` +
      `TTL: ${ttl}s | Cost: $${response.cost.toFixed(4)} | ` +
      `Embedding: $${embeddingCost.toFixed(6)}`
    );

    return cacheKey;
  } catch (error: any) {
    console.error('[Semantic Cache] Storage error:', error.message);
    throw error;
  }
}

// ============================================================================
// CACHE MANAGEMENT
// ============================================================================

/**
 * Invalidate specific cache entry
 */
export async function invalidateCacheEntry(cacheKey: string): Promise<void> {
  // If Redis is not available, skip caching
  if (!redis) {
    return;
  }

  try {
    await redis.del(cacheKey);
    console.log(`[Semantic Cache] üóëÔ∏è  DELETED - ${cacheKey}`);
  } catch (error: any) {
    console.error('[Semantic Cache] Delete error:', error.message);
  }
}

/**
 * Clear all semantic cache entries
 */
export async function clearSemanticCache(): Promise<number> {
  // If Redis is not available, skip caching
  if (!redis) {
    return 0;
  }

  try {
    const keys = await redis.keys(`${CACHE_KEY_PREFIX}*`);
    if (keys.length > 0) {
      await redis.del(...keys);
      console.log(`[Semantic Cache] üßπ CLEARED - ${keys.length} entries`);
    }
    return keys.length;
  } catch (error: any) {
    console.error('[Semantic Cache] Clear error:', error.message);
    return 0;
  }
}

/**
 * Remove expired cache entries
 */
export async function cleanupExpiredEntries(): Promise<number> {
  // If Redis is not available, skip caching
  if (!redis) {
    return 0;
  }

  try {
    const keys = await redis.keys(`${CACHE_KEY_PREFIX}*`);
    let deletedCount = 0;
    const now = Date.now();

    for (const key of keys) {
      const data = await redis.get(key);
      if (!data) continue;

      const entry: SemanticCacheEntry = JSON.parse(data);
      if (entry.metadata.expiresAt < now) {
        await redis.del(key);
        deletedCount++;
      }
    }

    if (deletedCount > 0) {
      console.log(`[Semantic Cache] üßπ CLEANUP - Removed ${deletedCount} expired entries`);
    }

    return deletedCount;
  } catch (error: any) {
    console.error('[Semantic Cache] Cleanup error:', error.message);
    return 0;
  }
}

/**
 * Get cache size (number of entries)
 */
export async function getCacheSize(): Promise<number> {
  // If Redis is not available, return 0
  if (!redis) {
    return 0;
  }

  try {
    const keys = await redis.keys(`${CACHE_KEY_PREFIX}*`);
    return keys.length;
  } catch (error: any) {
    console.error('[Semantic Cache] Size check error:', error.message);
    return 0;
  }
}

// ============================================================================
// STATISTICS & ANALYTICS
// ============================================================================

/**
 * Get comprehensive cache statistics
 */
export async function getSemanticCacheStats(): Promise<SemanticCacheStats> {
  const total = cacheHits + cacheMisses;
  const hitRate = total > 0 ? (cacheHits / total) * 100 : 0;
  const averageSimilarity = similarityCount > 0 ? totalSimilaritySum / similarityCount : 0;
  const cacheSize = await getCacheSize();

  return {
    hits: cacheHits,
    misses: cacheMisses,
    total,
    hitRate: `${hitRate.toFixed(2)}%`,
    totalCostSaved: parseFloat(totalCostSaved.toFixed(4)),
    totalTimeSaved,
    averageSimilarity: parseFloat(averageSimilarity.toFixed(4)),
    cacheSize,
  };
}

/**
 * Reset cache statistics
 */
export function resetCacheStats(): void {
  cacheHits = 0;
  cacheMisses = 0;
  totalCostSaved = 0;
  totalTimeSaved = 0;
  totalSimilaritySum = 0;
  similarityCount = 0;
  console.log('[Semantic Cache] üìä STATS RESET');
}

/**
 * Get detailed cache entry information
 */
export async function getCacheEntryDetails(cacheKey: string): Promise<SemanticCacheEntry | null> {
  // If Redis is not available, return null
  if (!redis) {
    return null;
  }

  try {
    const data = await redis.get(cacheKey);
    if (!data) return null;
    return JSON.parse(data);
  } catch (error: any) {
    console.error('[Semantic Cache] Entry retrieval error:', error.message);
    return null;
  }
}

/**
 * List all cache entries with metadata
 */
export async function listAllCacheEntries(): Promise<Array<{ key: string; entry: SemanticCacheEntry }>> {
  // If Redis is not available, return empty array
  if (!redis) {
    return [];
  }

  try {
    const keys = await redis.keys(`${CACHE_KEY_PREFIX}*`);
    const entries: Array<{ key: string; entry: SemanticCacheEntry }> = [];

    for (const key of keys) {
      const data = await redis.get(key);
      if (data) {
        entries.push({
          key,
          entry: JSON.parse(data),
        });
      }
    }

    return entries;
  } catch (error: any) {
    console.error('[Semantic Cache] List entries error:', error.message);
    return [];
  }
}

// ============================================================================
// HEALTH & DIAGNOSTICS
// ============================================================================

/**
 * Test Redis connection
 */
export async function testRedisConnection(): Promise<boolean> {
  // If Redis is not configured, return false
  if (!redis) {
    return false;
  }

  try {
    await redis.ping();
    return true;
  } catch (error) {
    console.error('[Semantic Cache] Redis connection failed:', error);
    return false;
  }
}

/**
 * Test OpenAI embeddings API
 */
export async function testEmbeddingAPI(): Promise<boolean> {
  try {
    await generateEmbedding('test prompt');
    return true;
  } catch (error) {
    console.error('[Semantic Cache] Embedding API failed:', error);
    return false;
  }
}

/**
 * Run comprehensive health check
 */
export async function healthCheck(): Promise<{
  redis: boolean;
  embeddings: boolean;
  cacheSize: number;
  stats: SemanticCacheStats;
}> {
  const [redis, embeddings, cacheSize, stats] = await Promise.all([
    testRedisConnection(),
    testEmbeddingAPI(),
    getCacheSize(),
    getSemanticCacheStats(),
  ]);

  return { redis, embeddings, cacheSize, stats };
}

// ============================================================================
// EXPORTS
// ============================================================================

export const SemanticCacheService = {
  // Core functionality
  search: searchSemanticCache,
  store: storeInSemanticCache,
  generateEmbedding,

  // Management
  invalidate: invalidateCacheEntry,
  clear: clearSemanticCache,
  cleanup: cleanupExpiredEntries,

  // Analytics
  getStats: getSemanticCacheStats,
  resetStats: resetCacheStats,
  getSize: getCacheSize,
  getEntry: getCacheEntryDetails,
  listAll: listAllCacheEntries,

  // Health
  healthCheck,
  testRedis: testRedisConnection,
  testEmbeddings: testEmbeddingAPI,

  // Constants
  EMBEDDING_MODEL,
  DEFAULT_SIMILARITY_THRESHOLD,
  DEFAULT_TTL,
};

export default SemanticCacheService;